<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="google-site-verification" content="oYFrTwOkCV4A4Szyb1Se3OctAeO3UY3Icw7z2iNJC3o"><meta><title>KV cache quantization - Hexo</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hexo"><meta name="msapplication-TileImage" content="/img/R-C.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hexo"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="kv cache 的简介以及量化技术概览"><meta property="og:type" content="blog"><meta property="og:title" content="KV cache quantization"><meta property="og:url" content="https://zengxw21.github.io/2025/03/23/KV-cache-quantization/"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="kv cache 的简介以及量化技术概览"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://zengxw21.github.io/gallery/kv/gpt2.jpg"><meta property="og:image" content="https://zengxw21.github.io/gallery/kv/attn.jpg"><meta property="article:published_time" content="2025-03-23T08:22:39.000Z"><meta property="article:modified_time" content="2025-04-04T17:32:47.042Z"><meta property="article:author" content="zxw"><meta property="article:tag" content="LLM inference"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://zengxw21.github.io/gallery/kv/gpt2.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://zengxw21.github.io/2025/03/23/KV-cache-quantization/"},"headline":"KV cache quantization","image":["https://zengxw21.github.io/gallery/kv/gpt2.jpg","https://zengxw21.github.io/gallery/kv/attn.jpg"],"datePublished":"2025-03-23T08:22:39.000Z","dateModified":"2025-04-04T17:32:47.042Z","author":{"@type":"Person","name":"zxw"},"publisher":{"@type":"Organization","name":"Hexo","logo":{"@type":"ImageObject","url":"https://zengxw21.github.io/img/R-C.png"}},"description":"kv cache 的简介以及量化技术概览"}</script><link rel="canonical" href="https://zengxw21.github.io/2025/03/23/KV-cache-quantization/"><link rel="icon" href="/img/R-C.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/R-C.png" alt="Hexo" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/zengxw21"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2025-03-23T08:22:39.000Z" title="2025-03-23T08:22:39.000Z">2025-03-23</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2025-04-04T17:32:47.042Z" title="2025-04-04T17:32:47.042Z">2025-04-05</time></span><span class="level-item"><a class="link-muted" href="/categories/machine-learning/">machine learning</a></span><span class="level-item">6 分钟读完 (大约899个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">KV cache quantization</h1><div class="content"><p>kv cache 的简介以及量化技术概览<br><span id="more"></span></p>
<h2 id="1-为什么需要-KV-cache"><a href="#1-为什么需要-KV-cache" class="headerlink" title="1. 为什么需要 KV cache"></a>1. 为什么需要 KV cache</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/668931470">这篇文章</a> 比较好地解释了 KV cache 的正确性以及必要性。文章给出了 gpt2 的架构和推理流程图：</p>
<img src="/gallery/kv/gpt2.jpg" class="" title="GPT2">
<p>关键的一点在于，最后的 token 概率分布<strong>仅使用</strong>矩阵的最后一行来计算得到（3b1b 的视频也强调了这一点），那么仅看 decode 过程，最后一行的计算就只和图上黄色部分相关了。</p>
<p>那么在实践中，decode 时我们只需要最近生成的 token 的 hidden states。这也就是为什么，在 attention 中，查看 decode 过程中 hidden state 的维度时，会发现是 <code>[1, dim]</code> 而不是 <code>[seq_len,dim]</code>。</p>
<p>在 attention 模块中，观察最后一行（最近的 token 的 embedding）是如何得到的：</p>
<img src="/gallery/kv/attn.jpg" class="" title="GPT2">
<p>通俗地讲，在自回归生成的文字接龙游戏中，为了准确预测下一个 token，我们想让最后一个 token 充分吸收上文的 token 们的语义，而 K、V 矩阵正好储存了这些信息，这就是为什么每次 forward 都要使用 K、V 矩阵。</p>
<p>既然 K 和 V 会被重复利用，自然可以将它们保存起来避免重新计算，这就是 KV cache。</p>
<h2 id="2-KV-cache-的量化"><a href="#2-KV-cache-的量化" class="headerlink" title="2. KV cache 的量化"></a>2. KV cache 的量化</h2><p>随着上下文长度增长，推理时 KV cache 的显存占用也在变大。量化的本质是将一个 tensor 的元素从大范围映射到小范围，从而减少内存占用。接下来通过代码模拟 KV cache 的量化过程。将产生的 key 和 value 添加到 past_key_value 之前，做一次 pseudo quantization，即量化 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="1.181ex" role="img" focusable="false" viewBox="0 -511 1000 522"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g></g></g></svg></mjx-container> 反量化的过程，即可模拟量化的精度损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">query_states = self.q_proj(hidden_states)</span><br><span class="line">key_states = self.k_proj(hidden_states)</span><br><span class="line">value_states = self.v_proj(hidden_states)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">省略一些代码</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># quantization</span></span><br><span class="line"><span class="keyword">if</span> self.config.kv_bit &lt; <span class="number">16</span>:</span><br><span class="line">key_states = pseudo_quantize_tensor(key_states, n_bits=self.config.kv_bit, q_group_size=self.config.kv_group_size)</span><br><span class="line">value_states = pseudo_quantize_tensor(value_states, n_bits=self.config.kv_bit, q_group_size=self.config.kv_group_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> past_key_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line"><span class="comment"># sin and cos are specific to RoPE models; cache_position needed for the static cache</span></span><br><span class="line">cache_kwargs = {<span class="string">"sin"</span>: sin, <span class="string">"cos"</span>: cos, <span class="string">"cache_position"</span>: cache_position}</span><br><span class="line">key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)</span><br></pre></td></tr></table></figure>
<p>pseudo_quantize_tensor 定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pseudo_quantize_tensor</span>(<span class="params">tensor, n_bits=<span class="number">8</span>, zero_point=<span class="literal">True</span>, q_group_size=-<span class="number">1</span>, per_tensor=<span class="literal">False</span>, inplace=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    The basic quantization function for weight, activation and KV cache.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    org_tensor_shape = tensor.shape</span><br><span class="line">    <span class="keyword">if</span> q_group_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> org_tensor_shape[-<span class="number">1</span>] % q_group_size == <span class="number">0</span></span><br><span class="line">        tensor = tensor.reshape(-<span class="number">1</span>, q_group_size)</span><br><span class="line">    <span class="keyword">if</span> per_tensor:</span><br><span class="line">        tensor = tensor.reshape(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">assert</span> tensor.dim() == <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> zero_point:</span><br><span class="line">        max_val = tensor.amax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        min_val = tensor.amin(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        max_int = <span class="number">2</span>**n_bits - <span class="number">1</span></span><br><span class="line">        min_int = <span class="number">0</span></span><br><span class="line">        scales = (max_val - min_val).clamp(<span class="built_in">min</span>=<span class="number">1e-5</span>) / max_int</span><br><span class="line">        zeros = (-torch.<span class="built_in">round</span>(min_val / scales)).clamp_(min_int, max_int)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        max_val = tensor.<span class="built_in">abs</span>().amax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        max_val = max_val.clamp(<span class="built_in">min</span>=<span class="number">1e-5</span>)</span><br><span class="line">        max_int = <span class="number">2</span> ** (n_bits - <span class="number">1</span>) - <span class="number">1</span></span><br><span class="line">        min_int = -(<span class="number">2</span> ** (n_bits - <span class="number">1</span>))</span><br><span class="line">        scales = max_val / max_int</span><br><span class="line">        zeros = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> inplace:</span><br><span class="line">        (</span><br><span class="line">            (tensor.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros)</span><br><span class="line">        ).mul_(scales)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tensor = (</span><br><span class="line">            torch.clamp(torch.<span class="built_in">round</span>(tensor / scales) + zeros, min_int, max_int) - zeros</span><br><span class="line">        ) * scales</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> torch.isnan(tensor).<span class="built_in">sum</span>() == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    tensor = tensor.reshape(org_tensor_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return the quantized tonsor, the scaling factor and the zero point value</span></span><br><span class="line">    <span class="comment"># return tensor, scales.view(tensor.shape[0], -1), zeros.view(tensor.shape[0], -1)</span></span><br><span class="line">    <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure></p>
<p>对 llama2 的 kv cache 进行 int4 量化以后，在 wikitext2 上进行 perplexity 的计算，发现与原模型相比，困惑度下降不超过 0.1。</p>
<p>实际部署的时候，应该是使用 int 存储 kv cache，参与 decode 计算的时候才反量化回 bf16。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>KV cache quantization</p><p><a href="https://zengxw21.github.io/2025/03/23/KV-cache-quantization/">https://zengxw21.github.io/2025/03/23/KV-cache-quantization/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>zxw</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2025-03-23</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2025-04-05</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/LLM-inference/">LLM inference </a></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/03/20/soft-ROCE-experiment/"><span class="level-item">soft ROCE 实践</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.png" alt="Zeng Xianwei"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Zeng Xianwei</p><p class="is-size-6 is-block">Tsinghua Univ.</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing,China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">6</p></a></div></div><a class="level-item has-text-centered is-marginless" href="/categories"><div><p class="heading">分类</p><div><p class="title">2</p></div></div></a><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">4</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/zengxw21" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/zengxw21"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#1-为什么需要-KV-cache"><span class="level-left"><span class="level-item">1</span><span class="level-item">1. 为什么需要 KV cache</span></span></a></li><li><a class="level is-mobile" href="#2-KV-cache-的量化"><span class="level-left"><span class="level-item">2</span><span class="level-item">2. KV cache 的量化</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/computer-network/"><span class="level-start"><span class="level-item">computer network</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/machine-learning/"><span class="level-start"><span class="level-item">machine learning</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-23T08:22:39.000Z">2025-03-23</time></p><p class="title"><a href="/2025/03/23/KV-cache-quantization/">KV cache quantization</a></p><p class="categories"><a href="/categories/machine-learning/">machine learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-20T08:21:39.000Z">2025-03-20</time></p><p class="title"><a href="/2025/03/20/soft-ROCE-experiment/">soft ROCE 实践</a></p><p class="categories"><a href="/categories/computer-network/">computer network</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-03-03T03:08:52.000Z">2025-03-03</time></p><p class="title"><a href="/2025/03/03/Reinforcement-Learning-Notes-3/">Reinforcement Learning Notes 3</a></p><p class="categories"><a href="/categories/machine-learning/">machine learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-25T03:44:34.000Z">2025-02-25</time></p><p class="title"><a href="/2025/02/25/Reinforcement-Learning-Notes-2/">Reinforcement Learning Notes 2</a></p><p class="categories"><a href="/categories/machine-learning/">machine learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-02-22T09:22:41.000Z">2025-02-22</time></p><p class="title"><a href="/2025/02/22/Reinforcement-Learning-Notes/">Reinforcement Learning Notes</a></p><p class="categories"><a href="/categories/machine-learning/">machine learning</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/"><span class="level-start"><span class="level-item">2025</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/LLM-inference/"><span class="tag">LLM inference</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RDMA/"><span class="tag">RDMA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RL/"><span class="tag">RL</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/thoughts/"><span class="tag">thoughts</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/R-C.png" alt="Hexo" height="28"></a><p class="is-size-7"><span>&copy; 2025 zxw</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/custom.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>