{"posts":[{"title":"KV cache quantization","text":"kv cache 的简介以及量化技术概览 1. 为什么需要 KV cache这篇文章 比较好地解释了 KV cache 的正确性以及必要性。文章给出了 gpt2 的架构和推理流程图： 关键的一点在于，最后的 token 概率分布仅使用矩阵的最后一行来计算得到（3b1b 的视频也强调了这一点），那么仅看 decode 过程，最后一行的计算就只和图上黄色部分相关了。 那么在实践中，decode 时我们只需要最近生成的 token 的 hidden states。这也就是为什么，在 attention 中，查看 decode 过程中 hidden state 的维度时，会发现是 [1, dim] 而不是 [seq_len,dim]。 在 attention 模块中，观察最后一行（最近的 token 的 embedding）是如何得到的： 通俗地讲，在自回归生成的文字接龙游戏中，为了准确预测下一个 token，我们想让最后一个 token 充分吸收上文的 token 们的语义，而 K、V 矩阵正好储存了这些信息，这就是为什么每次 forward 都要使用 K、V 矩阵。 既然 K 和 V 会被重复利用，自然可以将它们保存起来避免重新计算，这就是 KV cache。 2. KV cache 的量化随着上下文长度增长，推理时 KV cache 的显存占用也在变大。量化的本质是将一个 tensor 的元素从大范围映射到小范围，从而减少内存占用。接下来通过代码模拟 KV cache 的量化过程。将产生的 key 和 value 添加到 past_key_value 之前，做一次 pseudo quantization，即量化 反量化的过程，即可模拟量化的精度损失 1234567891011121314151617query_states = self.q_proj(hidden_states)key_states = self.k_proj(hidden_states)value_states = self.v_proj(hidden_states)'''省略一些代码'''# quantizationif self.config.kv_bit &lt; 16:key_states = pseudo_quantize_tensor(key_states, n_bits=self.config.kv_bit, q_group_size=self.config.kv_group_size)value_states = pseudo_quantize_tensor(value_states, n_bits=self.config.kv_bit, q_group_size=self.config.kv_group_size)if past_key_value is not None:# sin and cos are specific to RoPE models; cache_position needed for the static cachecache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs) pseudo_quantize_tensor 定义如下：12345678910111213141516171819202122232425262728293031323334353637383940414243@torch.no_grad()def pseudo_quantize_tensor(tensor, n_bits=8, zero_point=True, q_group_size=-1, per_tensor=False, inplace=False): \"\"\" The basic quantization function for weight, activation and KV cache. \"\"\" org_tensor_shape = tensor.shape if q_group_size &gt; 0: assert org_tensor_shape[-1] % q_group_size == 0 tensor = tensor.reshape(-1, q_group_size) if per_tensor: tensor = tensor.reshape(1, -1) assert tensor.dim() == 2 if zero_point: max_val = tensor.amax(dim=1, keepdim=True) min_val = tensor.amin(dim=1, keepdim=True) max_int = 2**n_bits - 1 min_int = 0 scales = (max_val - min_val).clamp(min=1e-5) / max_int zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int) else: max_val = tensor.abs().amax(dim=1, keepdim=True) max_val = max_val.clamp(min=1e-5) max_int = 2 ** (n_bits - 1) - 1 min_int = -(2 ** (n_bits - 1)) scales = max_val / max_int zeros = 0 if inplace: ( (tensor.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros) ).mul_(scales) else: tensor = ( torch.clamp(torch.round(tensor / scales) + zeros, min_int, max_int) - zeros ) * scales assert torch.isnan(tensor).sum() == 0 tensor = tensor.reshape(org_tensor_shape) # return the quantized tonsor, the scaling factor and the zero point value # return tensor, scales.view(tensor.shape[0], -1), zeros.view(tensor.shape[0], -1) return tensor 对 llama2 的 kv cache 进行 int4 量化以后，在 wikitext2 上进行 perplexity 的计算，发现与原模型相比，困惑度下降不超过 0.1。 实际部署的时候，应该是使用 int 存储 kv cache，参与 decode 计算的时候才反量化回 bf16。","link":"/2025/03/23/KV-cache-quantization/"},{"title":"Reinforcement Learning Notes 2","text":"Monte Carlo Method、随机梯度下降、时序差分方法 1. 蒙特卡罗方法1.1 基本思想在进行 policy iteration 时，求某个状态 s 采取动作 a 产生的 action value 的时候，我们已知的方法是代入公式 计算。但是这个方法依赖于模型参数已知，我们现在想找到一种方法，在不依赖模型的情况下能求出 action value。 为了达成这个目标，只需要对 policy iteration 每次迭代的第一步做改动。对每一个 pair ，生成充分多的序列 (episode)，对这些序列的 returns 求平均值，用来近似 action value。 1.2 MC Exploring Starts看这样一条 episode： 上述基本方法中，只有每条 episode 开头的 pair 的 action value 会被估计，这显然对数据利用不充分。于是产生了下面的改进： 对每一个 state-action-pair，只取一条 episode，对这个 episode 上出现过的逐个 pair 都做 action value 的计算，而且 policy 是随着每次计算而更新。这相当于，假如又出现了一次，则的估计又增添了样本，policy 也可能随着改动。 1.3 MC - Greedy我们不需要对每一个 pair 生成一条 episode，而是修改 policy 的更新方法，使其兼顾“explore”的功能。具体来说，我们将其改为 代表在 s 可选的 action 数量，位于 [0,1]。等于 0 时，就退化为之前的 greedy 策略；等于 1 时，选取每一个 action 的概率都相同，探索能力最强。 实际使用时，我们在每次迭代都生成足够长的 episode（e.g. 1million），用这条 episode 的每一个 step 一步步去更新 policy，只需数次迭代就可以收敛到较好的策略。 2. 随机梯度下降2.1 背景知识现在有一个数列，要计算它的期望，已知的方法是求平均值，但是在这要等到所有数据准备完毕。为了节约时间，可以采用迭代的 mean estimation 算法： 2.2 RM 算法要求解 的解 ，但是不知道 表达式，只知道给一个输入 ，会有一个输出 ，其中 代表噪音或误差。RM 算法给出如下迭代公式： 2.3 SGD考虑求的极小值。已知 是一个随机变量，但是不知道其分布；另外，已知梯度的采样。SGD 给出 这其实与机器学习的随机梯度下降原理一样。此外，还可以考虑批量梯度下降（BGD）、小批量梯度下降（MBGD）。 3. TD Learning3.1 基础算法我们要解决的问题是，给定一个策略，如何求解对应的 state value 。而且这个问题必须用 model free 的方法求解。 我们现在通过当前策略，生成了一个 episode，TD 算法给出渐进式的估计： 称为 TD Target，为 TD Error。 之所以称为 Target，是因为如下推导： 可以发现与的距离越来越小。 之所以称为 Error，是因为当是精确解时， 值得注意的是，上述算法仅是求一个给定策略的 state value，不涉及策略的优化。 3.2 Sarsa3.2.1 基础算法基础的 TD 算法是求 state value，而 Sarsa 求的是 action value。对一个给定的策略，我们收集当前状态下的 experience：。对 action value 的估计如下： 其中， 是对 的估计。 然后更新 处的策略： 然后跳到 进行下一步迭代。 3.2.2 n-Step Sarsa这是对 Sarsa 和 Monte Carlo 的折中： 回顾 action value 的定义： 是 discounted return。 在 Sarsa 中，它表示为 在 Monte Carlo 中，它表示为 在 n-Step Sarsa 中，它表示为 相应地，估计 action value 的公式改为 Sarsa 作为一种 online 的强化学习算法，每次只需要看下一步的 state 和 action，因此具有比较大的偏差（bias）和比较小的方差（variance）。MC 作为 offline 的算法，需要等待 episode 生成完成才能开始计算，具有较小的 bias 和较大的 variance。 多说一句，variance 较大意味着泛化性差，在这里体现为仅对一条 trajectory 进行拟合。 而 n-Step Sarsa 介于两者之间，需要到时刻才能计算。 Tips：在算法开始时，所有状态-动作对的 Q 值会被赋予初始值（如全零或随机数），并存储在一个表格（Q 表）中。 3.2.3 Q-LearningQ-Learning 的具体公式为 和 Sarsa 的区别在于 TD target 做了修改。 on policy vs off policy TD learning 里面有两个策略的概念：behavior policy，用于生成 experience；target policy，是要优化的对象。当两者相同时，成为 on policy，不同时成为 off policy。 off policy 的优势在于，target policy 要学习的 experience 不必是自己产生的。也就是说，允许从别人的经验中使自己提升。 Sarsa 和 MC 都是 target policy 自己生成数据，然后求出 action value 并优化策略，可见它们是 on policy 的。而 Q-Learning 计算需要，可以从任何一条策略产生，它是 off policy 的。 实际上，我们可以让 behavior 和 target 是同一个策略，来让 Q-Learning 是 on policy 的。这时，策略的更新需要使用方法；如果是 off policy，则使用 Greedy 方法，即每一步更新后的策略都是固定的。","link":"/2025/02/25/Reinforcement-Learning-Notes-2/"},{"title":"soft ROCE 实践","text":"使用 qemu 启动虚拟机，运行 perftest 的实践过程。 1. 环境搭建1.1 网络桥接设置（Host 端）为了让两个虚拟机可以互相通信，并且能够获得在局域网内的 IP 地址，需要创建一个网桥（例如 br0）。 下面给出命令行创建网桥和 TAP 设备的示例：12345678910111213# 创建网桥 br0sudo ip link add name br0 type bridgesudo ip addr add 192.168.100.1/24 dev br0sudo ip link set br0 up# 创建两个 TAP 设备（分别用于两个 VM）sudo ip tuntap add dev tap0 mode tap user $(whoami)sudo ip link set tap0 upsudo ip link set tap0 master br0sudo ip tuntap add dev tap1 mode tap user $(whoami)sudo ip link set tap1 upsudo ip link set tap1 master br0完成后，命令brctl show应该能看到 br0 关联了 tap0，tap1 两个设备。 1.2 虚拟机镜像制作这里选择使用源码编译的方法来制作镜像。在 kernel.org 上选择自己喜欢的版本下载下来。 然后运行如下命令123cd $KERNELmake defconfigmake kvm_guest.config 在命令行make menuconfig打开图形界面，选择需要的编译选项开启。这里需要开启 RDMA 的支持：Device Drivers-Infiniband support 里面全选（用 Y 选而不是 M）。保存退出后，执行make olddefconfig更新。 TIPS:查找编译选项时，可以键入”/“，即可快速搜索特定的关键词 然后编译：make -j ，完成后可以 check 一下：1234ls $KERNEL/vmlinux# sample output - $KERNEL/vmlinuxls $KERNEL/arch/x86/boot/bzImage# sample output - $KERNEL/arch/x86/boot/bzImage 然后安装debootstrap，新建$IMAGE文件夹，新建文件create-image.sh，内容如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222#!/usr/bin/env bash# Copyright 2016 syzkaller project authors. All rights reserved.# Use of this source code is governed by Apache 2 LICENSE that can be found in the LICENSE file.# create-image.sh creates a minimal Debian Linux image suitable for syzkaller.set -eux# Create a minimal Debian distribution in a directory.PREINSTALL_PKGS=openssh-server,curl,tar,gcc,libc6-dev,time,strace,sudo,less,psmisc,selinux-utils,policycoreutils,checkpolicy,selinux-policy-default,firmware-atheros,debian-ports-archive-keyring,libibverbs1,ibverbs-utils,librdmacm1,libibumad3,ibverbs-providers,rdma-core,rdmacm-utils,wget,perftest,iproute2# If ADD_PACKAGE is not defined as an external environment variable, use our default packagesif [ -z ${ADD_PACKAGE+x} ]; then ADD_PACKAGE=&quot;make,sysbench,git,vim,tmux,usbutils,tcpdump&quot;fi# Variables affected by optionsARCH=$(uname -m)RELEASE=bullseyeFEATURE=minimalSEEK=2047PERF=false# Display help functiondisplay_help() { echo &quot;Usage: $0 [option...] &quot; &gt;&amp;2 echo echo &quot; -a, --arch Set architecture&quot; echo &quot; -d, --distribution Set on which debian distribution to create&quot; echo &quot; -f, --feature Check what packages to install in the image, options are minimal, full&quot; echo &quot; -s, --seek Image size (MB), default 2048 (2G)&quot; echo &quot; -h, --help Display help message&quot; echo &quot; -p, --add-perf Add perf support with this option enabled. Please set envrionment variable \\$KERNEL at first&quot; echo}while true; do if [ $# -eq 0 ];then echo $# break fi case &quot;$1&quot; in -h | --help) display_help exit 0 ;; -a | --arch) ARCH=$2 shift 2 ;; -d | --distribution) RELEASE=$2 shift 2 ;; -f | --feature) FEATURE=$2 shift 2 ;; -s | --seek) SEEK=$(($2 - 1)) shift 2 ;; -p | --add-perf) PERF=true shift 1 ;; -*) echo &quot;Error: Unknown option: $1&quot; &gt;&amp;2 exit 1 ;; *) # No more options break ;; esacdone# Handle cases where qemu and Debian use different arch namescase &quot;$ARCH&quot; in ppc64le) DEBARCH=ppc64el ;; aarch64) DEBARCH=arm64 ;; arm) DEBARCH=armel ;; x86_64) DEBARCH=amd64 ;; *) DEBARCH=$ARCH ;;esac# Foreign architectureFOREIGN=falseif [ $ARCH != $(uname -m) ]; then # i386 on an x86_64 host is exempted, as we can run i386 binaries natively if [ $ARCH != &quot;i386&quot; -o $(uname -m) != &quot;x86_64&quot; ]; then FOREIGN=true fifiif [ $FOREIGN = &quot;true&quot; ]; then # Check for according qemu static binary if ! which qemu-$ARCH-static; then echo &quot;Please install qemu static binary for architecture $ARCH (package 'qemu-user-static' on Debian/Ubuntu/Fedora)&quot; exit 1 fi # Check for according binfmt entry if [ ! -r /proc/sys/fs/binfmt_misc/qemu-$ARCH ]; then echo &quot;binfmt entry /proc/sys/fs/binfmt_misc/qemu-$ARCH does not exist&quot; exit 1 fifi# Double check KERNEL when PERF is enabledif [ $PERF = &quot;true&quot; ] &amp;&amp; [ -z ${KERNEL+x} ]; then echo &quot;Please set KERNEL environment variable when PERF is enabled&quot; exit 1fi# If full feature is chosen, install more packagesif [ $FEATURE = &quot;full&quot; ]; then PREINSTALL_PKGS=$PREINSTALL_PKGS&quot;,&quot;$ADD_PACKAGEfiDIR=$RELEASEsudo rm -rf $DIRsudo mkdir -p $DIRsudo chmod 0755 $DIR# 1. debootstrap stageDEBOOTSTRAP_PARAMS=&quot;--arch=$DEBARCH --include=$PREINSTALL_PKGS --components=main,contrib,non-free,non-free-firmware $RELEASE $DIR&quot;if [ $FOREIGN = &quot;true&quot; ]; then DEBOOTSTRAP_PARAMS=&quot;--foreign $DEBOOTSTRAP_PARAMS&quot;fi# riscv64 is hosted in the debian-ports repository# debian-ports doesn't include non-free, so we exclude firmware-atherosif [ $DEBARCH == &quot;riscv64&quot; ]; then DEBOOTSTRAP_PARAMS=&quot;--keyring /usr/share/keyrings/debian-ports-archive-keyring.gpg --exclude firmware-atheros $DEBOOTSTRAP_PARAMS http://deb.debian.org/debian-ports&quot;fi# debootstrap may fail for EoL Debian releasesRET=0sudo --preserve-env=http_proxy,https_proxy,ftp_proxy,no_proxy debootstrap $DEBOOTSTRAP_PARAMS || RET=$?if [ $RET != 0 ] &amp;&amp; [ $DEBARCH != &quot;riscv64&quot; ]; then # Try running debootstrap again using the Debian archive DEBOOTSTRAP_PARAMS=&quot;--keyring /usr/share/keyrings/debian-archive-removed-keys.gpg $DEBOOTSTRAP_PARAMS https://mirrors.tuna.tsinghua.edu.cn/debian/&quot; sudo --preserve-env=http_proxy,https_proxy,ftp_proxy,no_proxy debootstrap $DEBOOTSTRAP_PARAMSfi# 2. debootstrap stage: only necessary if target != host architectureif [ $FOREIGN = &quot;true&quot; ]; then sudo cp $(which qemu-$ARCH-static) $DIR/$(which qemu-$ARCH-static) sudo chroot $DIR /bin/bash -c &quot;/debootstrap/debootstrap --second-stage&quot;fi# Set some defaults and enable promtless ssh to the machine for root.sudo sed -i '/^root/ { s/:x:/::/ }' $DIR/etc/passwdecho 'T0:23:respawn:/sbin/getty -L ttyS0 115200 vt100' | sudo tee -a $DIR/etc/inittabprintf '\\nauto eth0\\niface eth0 inet dhcp\\n' | sudo tee -a $DIR/etc/network/interfacesecho '/dev/root / ext4 defaults 0 0' | sudo tee -a $DIR/etc/fstabecho 'debugfs /sys/kernel/debug debugfs defaults 0 0' | sudo tee -a $DIR/etc/fstabecho 'securityfs /sys/kernel/security securityfs defaults 0 0' | sudo tee -a $DIR/etc/fstabecho 'configfs /sys/kernel/config/ configfs defaults 0 0' | sudo tee -a $DIR/etc/fstabecho 'binfmt_misc /proc/sys/fs/binfmt_misc binfmt_misc defaults 0 0' | sudo tee -a $DIR/etc/fstabecho -en &quot;127.0.0.1\\tlocalhost\\n&quot; | sudo tee $DIR/etc/hostsecho &quot;nameserver 8.8.8.8&quot; | sudo tee -a $DIR/etc/resolv.confecho &quot;syzkaller&quot; | sudo tee $DIR/etc/hostnamessh-keygen -f $RELEASE.id_rsa -t rsa -N ''sudo mkdir -p $DIR/root/.ssh/cat $RELEASE.id_rsa.pub | sudo tee $DIR/root/.ssh/authorized_keys# Add perf supportif [ $PERF = &quot;true&quot; ]; then cp -r $KERNEL $DIR/tmp/ BASENAME=$(basename $KERNEL) sudo chroot $DIR /bin/bash -c &quot;apt-get update; apt-get install -y flex bison python-dev libelf-dev libunwind8-dev libaudit-dev libslang2-dev libperl-dev binutils-dev liblzma-dev libnuma-dev&quot; sudo chroot $DIR /bin/bash -c &quot;cd /tmp/$BASENAME/tools/perf/; make&quot; sudo chroot $DIR /bin/bash -c &quot;cp /tmp/$BASENAME/tools/perf/perf /usr/bin/&quot; rm -r $DIR/tmp/$BASENAMEfi# Add udev rules for custom drivers.# Create a /dev/vim2m symlink for the device managed by the vim2m driverecho 'ATTR{name}==&quot;vim2m&quot;, SYMLINK+=&quot;vim2m&quot;' | sudo tee -a $DIR/etc/udev/rules.d/50-udev-default.rules# Add udev rules for RDMA devices.echo 'KERNEL==&quot;uverbs*&quot;, MODE=&quot;0666&quot;' | sudo tee -a $DIR/etc/udev/rules.d/99-rdma.rulesecho 'KERNEL==&quot;umad*&quot;, MODE=&quot;0666&quot;' | sudo tee -a $DIR/etc/udev/rules.d/99-rdma.rules# sudo chroot $DIR /bin/bash -c &quot;udevadm control --reload&quot;# Set up RXE interfaces.sudo chroot $DIR /bin/bash -c &quot;cat &gt; /etc/systemd/system/rdma-init.service &lt;&lt; EOF[Unit]Description=Initialize Soft-RoCE (rxe)After=network.target[Service]Type=oneshotExecStart=/usr/bin/rdma link add rxe_0 type rxe netdev eth0[Install]WantedBy=multi-user.targetEOF&quot;sudo chroot $DIR /bin/bash -c &quot;systemctl enable rdma-init.service&quot;# Build a disk imagedd if=/dev/zero of=$RELEASE.img bs=1M seek=$SEEK count=1sudo mkfs.ext4 -F $RELEASE.imgsudo mkdir -p /mnt/$DIRsudo mount -o loop $RELEASE.img /mnt/$DIRsudo cp -a $DIR/. /mnt/$DIR/.sudo umount /mnt/$DIR 因为自己的虚拟机稍后要用于 fuzz，所以在 syzkaller 给的脚本上稍作了修改，效果是一样的 耐心等待镜像制作完成即可。 2. 虚拟机配置2.1 启动虚拟机按如下方式启动两个虚拟机（要求先安装 QEMU）： 虚拟机 1： 123456789101112qemu-system-x86_64 \\ -m 2G \\ -smp 2 \\ -kernel $KERNEL/arch/x86/boot/bzImage \\ -append &quot;console=ttyS0 root=/dev/sda earlyprintk=serial net.ifnames=0&quot; \\ -drive file=$IMAGE/bullseye_vm1.img,format=raw \\ -netdev tap,id=net0,ifname=tap0,script=no,downscript=no \\ -device e1000,netdev=net0,mac=52:54:00:12:34:56 \\ -enable-kvm \\ -nographic \\ -pidfile vm1.pid \\ 2&gt;&amp;1 | tee vm1.log 虚拟机 2： 123456789101112qemu-system-x86_64 \\ -m 2G \\ -smp 2 \\ -kernel $KERNEL/arch/x86/boot/bzImage \\ -append &quot;console=ttyS0 root=/dev/sda earlyprintk=serial net.ifnames=0&quot; \\ -drive file=$IMAGE/bullseye_vm2.img,format=raw \\ -netdev tap,id=net1,ifname=tap1,script=no,downscript=no \\ -device e1000,netdev=net1,mac=52:54:00:12:34:57 \\ -enable-kvm \\ -nographic \\ -pidfile vm2.pid \\ 2&gt;&amp;1 | tee vm2.log TIPS手动指定 mac 地址，否则后续数据包转发可能失败 2.2 网络配置在每个 Debian 虚拟机中配置网络，使其位于与主机桥接相同的子网内。编辑 /etc/network/interfaces（或使用 NetworkManager）配置，例如：12345auto eth0iface eth0 inet static address 192.168.100.101 # 对 VM1；VM2 可用 192.168.100.102 netmask 255.255.255.0 gateway 192.168.100.1重启网络服务或重启虚拟机以使配置生效：1sudo systemctl restart networking或使用：1sudo ifdown eth0 &amp;&amp; sudo ifup eth0 这时候互相 ping 对方应该能 ping 通。 3.ROCE setup and perftest3.1 检查 rxe 设备因为之前我们选择了 built-in 的方式来编译 rdma 模块，所以这里不用另外加载，直接检查：12ibv_devicesibv_devinfo上述命令应该会显示一些设备信息。例如执行ibv_devices时，结果如下： 在两台机器上分别执行上述命令，记下来设备名以供后续使用。 3.2 perftest我们在两端分别执行： ib_send_bw -d &lt;设备名&gt; 以及： ib_send_bw -d &lt;设备名&gt; &lt;server_ip&gt; server 端结果如下：","link":"/2025/03/20/soft-ROCE-experiment/"},{"title":"Reinforcement Learning Notes","text":"从入门到入坑 1. 贝尔曼公式考虑这样一个状态转移，agent 在状态 S 采取动作 A，获得 reward R 并转移到下一个状态： 我们计算整条路径上 reward 之和也就是 return 那么如何计算从 s 开始，采用策略获得的 return 的期望，也就是 state value 呢？它等于 分别计算这两项，第一项： 其中 表示在 policy 下，在状态 s 采取动作 a 的概率； 表示在状态 s 采取动作 a 后，获得奖励 reward 的概率。那么上面的式子其实就是求第一步 reward 的期望。 接下来看第二项， 其中 的原因是，整个过程是 Markov 决策，也就是与历史状态无关。而 指的是从 开始，能得到的 return 的期望。这一项求的是从下一个状态开始能得到的 return 的期望。 回头看开始的式子，提取出公共项，就有 注意这里的集合符号，代表这是一组等式，每个 state 都有一个方程。联立这个方程组，我们就能求出每个状态的 state value，进而评估这个策略的好坏。 接下来看 action value 的定义： 它指的是在状态 s，采取动作 a 后，获得 return 的期望。那么我们不难得出，state value 其实就是 action value 的加权平均。 将其与上面得到的式子作比较，发现 这说明 state value 和 action value 其实可以互相求解。 2. 贝尔曼最优公式 (BOE)如何评价一个策略是否是最优的？对于任意一个状态 s，在策略下的 state value 不小于任何其他策略在 s 的 state value，我们就说它是最优策略。 BOE 其实就是求最优策略和对应的 state value 的方法。 在上述公式中，、 都是未知的。这个式子的含义就是，如何选取策略，使得在 s 处的 state value 能达到最大。当然也可以写成矩阵形式 那么一个式子，两个变量，如何求解最优化问题？我们先固定 v，有 注意代表 action value。对于这个问题，一个朴素的想法是，哪个 action value 最大，就为它分配更多的权重 c。那么我们有 那么策略就是在 s 选取能让 action value 达到最大的动作。 好，那么问题就剩下如何求解 v。注意到 这里需要引入一条定理 ======================For any equation that has the form of x = f(x), if f is a contraction mapping,then• Existence: there exists a fixed point satisfying • Uniqueness: The fixed point is unique• Algorithm: Consider a sequence where , then as Moreover, the convergence rate is exponentially fast. ====================== 其实， 满足 ，即压缩映射的定义。那么我们就可以用不动点迭代的方法求出此时的最优 state value . 回过头看，最优策略就可以写为 前面说过，最优策略基于贪心思想，在 s 选取能让 action value 达到最大的动作，即 其中， 3. 值迭代与策略迭代3.1 value iteration已知 如何求解 state value？我们已经知道可以用迭代的方法， 实际上这个方法的每次迭代分为两个步骤。第一步，找出当前的对应的最优策略，然后根据求出。具体是怎么实现的呢？ 第一步， 也可以写成形式 上一章中提到，这个最优化问题的解是 然后把新的 policy 代入原式，得 也就是 因为最优策略只在最大的 action value 处选取，所以上式可以进一步简化为 这样我们就完成了一步 value iteration 3.2 policy iteration策略迭代中，每次迭代的第一步，是先对当前的进行迭代，找到合适的 迭代的目的其实是为了求解已知当前策略的贝尔曼方程 我们也可以把迭代写成 element wise form 当 j 充分大时，我们认为收敛到原方程的解。 迭代的第二步，是求解 也就是 根据前面的结论，不难得出 然后就可以用新的策略进行下一次迭代了。 3.3 truncated policy iteration比较上述两种方法，我们发现更新时 value iteration 只算了一步 而 policy iteration 理论上需要将新的反复代入原式迭代。在实际实现中，我们自然需要在某个迭代时停下来，这就是 truncated policy iteration。 事实上，value iteration 和 policy iteration 是 truncated policy iteration 的两个极端，而实际实现的 policy iteration 本质上是 truncated 的。","link":"/2025/02/22/Reinforcement-Learning-Notes/"},{"title":"Reinforcement Learning Notes 3","text":"值函数近似、策略梯度方法、Actor-Critic 1. 值函数近似之前计算、更新 state/action value 的方法都是基于离散的表格的，现在想用函数来对连续状态进行估计。 总体的思想是，用函数来拟合 state value，更新时，对参数 w 进行更新。 例如，要计算给定策略的 state value，用估计，对于每个 example，更新参数。这只是在求 state value，并没有涉及到 policy update，接下来的方法将直接拟合 action value 并更新参数，这样就可以得到最优策略。 1.1 Sarsa是对于 action value 的拟合。直接给出参数的更新公式： 每步迭代更新一次，然后使用的方法更新策略。 1.2 Q-Learningq-value 的更新与 Sarsa 很像： 策略更新还是用。 1.3 Deep Q-Network基本思想是，是一个神经网络，输入是 s，a，参数是 w。loss function 定义为 不难发现，这个 loss 的本质是 bellman error，如何使它最小化？问题在于这一项。当 w 变化时，让这一项达到最大的 a 也可能会变化。为了消除这种不确定性，DQN 提出了两个技巧。 1. 使用两个神经网络。一个是 main network ，另一个是 target network。target network 的参数不用每一次迭代都更新，在一段时间内我们假设它是固定的，方便求梯度。target network 固定时，梯度就可以写为 这样一来，对于样本，desired output 就是确定的数：，我们就获得了 mini-batch，可以用来最小化 loss function。 2.replay buffer。 DQN 是 off policy 的，也就是说样本可以由其它策略得到，我们选取样本时，不按照 episode 的顺序，而是打混、等概率随机抽取。这样的好处是，不需要生成太长的 episode，能够充分利用样本。 DQN 算法流程如下：对于样本，计算：，然后用这个 batch 来更新 main network 参数，以最小化 loss function。每过 C 个迭代，就更新一次即 target network 的参数。 policy update 等到算法结束后才一次性完成，因为中间并不需要使用 target policy。有了训练好的，就不难知道在某个状态 s 采取哪个动作的 action value 最大，也就很容易得到 optimal policy 了。 2. 策略梯度方法2.1 基本思想之前，在某个状态 s 要采取什么动作，都是通过查表的形式完成。现在想用一个函数（例如神经网络）来直接得出在 s 时采取动作 a 的概率，写为，是网络的参数。 2.2 目标函数如何更新参数？有两个常用的指标：和。第一个比较容易理解，重点看第二个。 ；而可以理解为初始 s 服从某种分布，假设将它写成向量 d，每一个元素就是。而这个向量 d 如何得到呢？假设我们有 state transition matrix ，d 就是的特征向量。 这些指标是的函数，也就是的函数。更新参数的目标就是让这些指标变得尽可能大。值得一提的是，当一个指标最大时，另外的指标也会跟着达到最大，也就是它们的优化是等价的。 有的时候，这两个目标函数也可以写为 第二个式子的第二个等号成立的原因是大数定律，也就是无穷多步以后每个状态的概率已经稳定，而且不考虑 discount。 2.3 REINFORCE接下来，就是使用梯度上升的方法来更新，以最大化目标函数。目标函数梯度的推导过程这里不详细给出，直接看 REINFORCE 算法。 现在有一个网络。在第 k 次迭代时，选取一个，从它开始生成 episode。对每一个状态对，做 ，即从开始算的 action value 最后，进入下一次迭代。 deeper insight 有趣的是， 这实际上是在”合理化”的值（回想梯度上升的公式，不就是的梯度吗），为正数时，经过参数调整后变大；为负数时，经过参数调整后变小。进一步观察，因为的最后一层有 softmax 操作，故分母是正数，那么的正负不就是 action value 的正负吗？ 另外，这个算法还兼顾了探索性。因为当太小时，（假设此时为正数）也会变大，参数更新后将变大，这使得某些可能性较低的情况也会被充分探索。 3.Actor-Criticactor-critic 也是一种策略梯度方法，actor 代表策略的更新，critic 代表策略的评估。基本思想仍然是用梯度上升的方法来最大化目标函数。 3.1 QAC回顾上一节的公式 可以发现，原来是通过采样得到的。现在想直接用神经网络来得到每个（s,a）的 action value。一个基本的算法是，每次迭代时，由策略和环境生成，然后用这个来进行： policy 更新： value 更新： 上述方法称为 QAC。 3.2 A2C目标函数的梯度具有如下性质： 减去有什么好处？这样可以减小方差，进而使采样时样本与期望的误差不至于太大，数学证明这里省略。实际我们常常让 b(s) 等于，即 s 的 state value。 这样一来，上述梯度可以写为 其中定义优势函数。 也可以写出针对某一个具体 episode 的版本： 优势函数还可以进一步改写为。这样做的好处是，原来需要两个网络分别估计 action value 和 state value，现在只需要训练一个网络估计 state value。 这样一来，就得到了 A2C 算法。里面有两个网络： 和 。具体流程是，每次迭代时，由策略和环境生成，然后用这个来进行： 计算优势函数： 更新策略： 更新价值： 可见，这是 online、on-policy 的算法。 deeper insight 越大，说明这一的 “优势” 越大，我们自然想让它被采取的概率变大，而数学上也确实变大。越小，基于充分探索的思想，也会变大。 接下来看一下优势函数评估某个 action 的好处。用这个代替 action value，使其更加关心 相对 大小。这是更加合理的。例如，这个动作产生的 action value 是 0，但是其它的都是-1，那么这个动作无疑是最优的。相反，即使这个动作的 aciton value 是 10，如果其它的都是 100，也不能说这个动作占优。 3.3 DPG3.3.1 重要性采样已知随机变量 X 服从分布，但是有时候不能直接求出它的期望（比如 X 是神经网络的输出）。同时还知道一个服从分布的采样。现在想要间接地求出 X 的期望，需要用到重要性采样： 3.3.2 off policy 梯度下降假设现在有一个 behavior policy ，想要用它的采样来更新 target policy ，以最大化目标函数 根据重要性采样原理，梯度可以写为 减去 baseline，我们就得到了 off policy 的参数更新公式： 定义优势函数后，改写为 其中 这样一来就得到了 off policy 的 A2C 算法，其中 sample 都是由 behavior policy 生成 计算优势函数： 更新策略： 更新价值： 3.3.3 DPG之前对于一个状态，网络会输出采取某个动作的概率。现在想要直接从状态映射到动作本身：。 目标函数选，梯度写为 这样一来，就得到了DPG算法。假设有一个behavior policy ，target policy ，评估函数 ，还是三件套： 优势函数 策略更新 评估更新 注意到这里不需要重要性采样了。","link":"/2025/03/03/Reinforcement-Learning-Notes-3/"},{"title":"自己需要记住的一些事","text":"05cfa70b99313444de02ee59215d007d525a4ddde384c9cadfce0102ae417bee127747e34e6dbb8747869b3c2f82c2757435566e505158e1c187f2095b17cfda8b1942e73f5e01efc43411def04f5104454f443f0a82b30531b027fb88514f58af464ffb102cba2c4b6ff393b29fcc7191a8fe883d5c51675f53cba1a8e00615019a631763b806e3cdcac2ec4a735b8450c2646726561c178ef05531836f0849f985790b6b2bf7e6e020fc2af0007ee629a6c90f58f6904d6dccb9731a65c32a2cde4584bc8f0d82377ddcea8af3a60d34ac5d0e30a67a5af673a972640ddd9c427c1f20dae9c70724654c2516f059b379813f35e4aab37d73eab4609d6222e1a8d7f9d5f6408d0d9ba35c0bb8464d9ce5572064e0075ee3536edfcb3598aac9f4b3e0bed2bb12866033992c62afa004274b5aec63016a5deb706a8dc7bd3060cba49afc97b73a91bd8bd356ca34e816f91066d8eb06ad3a8dc51f60dcd40166aead46c734c7eaf68c8bb4205d3ad8bdb6c3f20813e68dffc65bf8e87ff643d31195e8df2e002f8eea1c77120467c7ecc349b2cffe34582c9217708417d57dac497e24227712588ec52fdf4fdd0b08a9d41228ae5eb97857fa95fcee6f863c3c012c36c20689e63b191e00f3679468593d0312ed1e1b069c56a3f8757f058a8938a17ba8d86057914bdfb008d56d0a6dcbe3b0d6b33da6033d13db2adfbdcbb5782117276dcfb2d35a24bef1b1dca557c9d02a7788c225dbebdc944f69968f9ceeeef07af3a7c0491319e0ea04ce320613b185db8432c24fc3f49849b7b9fc945f7704091fad9f3ba1baecdd809cdc4a7966d72f275302cb8f7934b10af0e8aebc18f5452850a35707a401deeacd120e0bd761bd808697ff0a80797f849c16c2bfe2f50c5af6a6c05c539520d8b0ae6a4fdfa43018787c37de6e663cd394df70cf3f1a8484fd9982f570887f2d922daed006218fe567f74bbc33e6568b53b0e45a490b8b36c58bb7ee7964527720ef958985b0445ea2105b592d221946801fe19ed7379719d9cb4b7ba59d0de2e8be86327238c915c4ea0a2285230dd81f11c6f087085b98ff5ebf48c0ea8301836f1b 当前文章暂不对外可见，请输入访问密码后查看！","link":"/2024/01/09/%E8%87%AA%E5%B7%B1%E9%9C%80%E8%A6%81%E8%AE%B0%E4%BD%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/"},{"title":"RLHF Overview","text":"basic concepts about RLHF… Reward Modeling 是一个打分模型，评估模型输出答案的好坏。 架构：在语言模型后面加上一个 linear head。 训练时，用于模仿人类偏好，在两个候选答案中选出更符合人类偏好的那一个。 训练 Reward Model 的时候，对于同一个 prompt ，有两个候选答案，，若更加被偏好，Reward Model 需要让它的相对分数更高，即 maximize 训练时，模型参数优化下面的函数 ORM vs PRM通俗来说，ORM 评估模型的输出的时候，只看模型给的答案对不对，所以可以视为训练一个二分类器，损失函数为二分类交叉熵 ，是人工给答案打上的标签。 PRM 评估模型输出时更加关注过程的正确性。训练数据是题目+reasoning steps，每个 step 都有人工的打标（-1 for incorrect, 0 for neutral, 1 for correct），PRM 会在遇到特定的 token 时打分（例如遇到代表一句推理结束的\\n\\n），这时打分就是对前面整句推理的打分了，训练时候将每句推理都视为一个特殊的 “token”，以 next token prediction 的方式进行 autoregressive 的分数生成。","link":"/2025/10/14/RLHF-Overview/"}],"tags":[{"name":"LLM inference","slug":"LLM-inference","link":"/tags/LLM-inference/"},{"name":"RL","slug":"RL","link":"/tags/RL/"},{"name":"RDMA","slug":"RDMA","link":"/tags/RDMA/"},{"name":"thoughts","slug":"thoughts","link":"/tags/thoughts/"}],"categories":[{"name":"machine learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"computer network","slug":"computer-network","link":"/categories/computer-network/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"logo是dropout bear，头像是泽北荣治。喜欢学习机器学习相关的内容。","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}