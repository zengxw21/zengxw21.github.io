{"posts":[{"title":"Reinforcement Learning Notes","text":"从入门到入坑 1. 贝尔曼公式考虑这样一个状态转移，agent 在状态 S 采取动作 A，获得 reward R 并转移到下一个状态： 我们计算整条路径上 reward 之和也就是 return 那么如何计算从 s 开始，采用策略获得的 return 的期望，也就是 state value 呢？它等于 分别计算这两项，第一项： 其中 表示在 policy 下，在状态 s 采取动作 a 的概率； 表示在状态 s 采取动作 a 后，获得奖励 reward 的概率。那么上面的式子其实就是求第一步 reward 的期望。 接下来看第二项， 其中 的原因是，整个过程是 Markov 决策，也就是与历史状态无关。而 指的是从 开始，能得到的 return 的期望。这一项求的是从下一个状态开始能得到的 return 的期望。 回头看开始的式子，提取出公共项，就有 注意这里的集合符号，代表这是一组等式，每个 state 都有一个方程。联立这个方程组，我们就能求出每个状态的 state value，进而评估这个策略的好坏。 接下来看 action value 的定义： 它指的是在状态 s，采取动作 a 后，获得 return 的期望。那么我们不难得出，state value 其实就是 action value 的加权平均。 将其与上面得到的式子作比较，发现 这说明 state value 和 action value 其实可以互相求解。 2. 贝尔曼最优公式 (BOE)如何评价一个策略是否是最优的？对于任意一个状态 s，在策略下的 state value 不小于任何其他策略在 s 的 state value，我们就说它是最优策略。 BOE 其实就是求最优策略和对应的 state value 的方法。 在上述公式中，、 都是未知的。这个式子的含义就是，如何选取策略，使得在 s 处的 state value 能达到最大。当然也可以写成矩阵形式 那么一个式子，两个变量，如何求解最优化问题？我们先固定 v，有 注意代表 action value。对于这个问题，一个朴素的想法是，哪个 action value 最大，就为它分配更多的权重 c。那么我们有 那么策略就是在 s 选取能让 action value 达到最大的动作。 好，那么问题就剩下如何求解 v。注意到 这里需要引入一条定理 ======================For any equation that has the form of x = f(x), if f is a contraction mapping,then• Existence: there exists a fixed point satisfying • Uniqueness: The fixed point is unique• Algorithm: Consider a sequence where , then as Moreover, the convergence rate is exponentially fast. ====================== 其实， 满足 ，即压缩映射的定义。那么我们就可以用不动点迭代的方法求出此时的最优 state value . 回过头看，最优策略就可以写为 前面说过，最优策略基于贪心思想，在 s 选取能让 action value 达到最大的动作，即 其中， 3. 值迭代与策略迭代3.1 value iteration已知 如何求解 state value？我们已经知道可以用迭代的方法， 实际上这个方法的每次迭代分为两个步骤。第一步，找出当前的对应的最优策略，然后根据求出。具体是怎么实现的呢？ 第一步， 也可以写成形式 上一章中提到，这个最优化问题的解是 然后把新的 policy 代入原式，得 也就是 因为最优策略只在最大的 action value 处选取，所以上式可以进一步简化为 这样我们就完成了一步 value iteration 3.2 policy iteration策略迭代中，每次迭代的第一步，是先对当前的进行迭代，找到合适的 迭代的目的其实是为了求解已知当前策略的贝尔曼方程 我们也可以把迭代写成 element wise form 当 j 充分大时，我们认为收敛到原方程的解。 迭代的第二步，是求解 也就是 根据前面的结论，不难得出 然后就可以用新的策略进行下一次迭代了。 3.3 truncated policy iteration比较上述两种方法，我们发现更新时 value iteration 只算了一步 而 policy iteration 理论上需要将新的反复代入原式迭代。在实际实现中，我们自然需要在某个迭代时停下来，这就是 truncated policy iteration。 事实上，value iteration 和 policy iteration 是 truncated policy iteration 的两个极端，而实际实现的 policy iteration 本质上是 truncated 的。","link":"/2025/02/22/Reinforcement-Learning-Notes/"},{"title":"自己需要记住的一些事","text":"05cfa70b99313444de02ee59215d007d525a4ddde384c9cadfce0102ae417bee127747e34e6dbb8747869b3c2f82c2757435566e505158e1c187f2095b17cfda8b1942e73f5e01efc43411def04f5104454f443f0a82b30531b027fb88514f58af464ffb102cba2c4b6ff393b29fcc7191a8fe883d5c51675f53cba1a8e00615019a631763b806e3cdcac2ec4a735b8450c2646726561c178ef05531836f0849f985790b6b2bf7e6e020fc2af0007ee629a6c90f58f6904d6dccb9731a65c32a2cde4584bc8f0d82377ddcea8af3a60d34ac5d0e30a67a5af673a972640ddd9c427c1f20dae9c70724654c2516f059b379813f35e4aab37d73eab4609d6222e1a8d7f9d5f6408d0d9ba35c0bb8464d9ce5572064e0075ee3536edfcb3598aac9f4b3e0bed2bb12866033992c62afa004274b5aec63016a5deb706a8dc7bd3060cba49afc97b73a91bd8bd356ca34e816f91066d8eb06ad3a8dc51f60dcd40166aead46c734c7eaf68c8bb4205d3ad8bdb6c3f20813e68dffc65bf8e87ff643d31195e8df2e002f8eea1c77120467c7ecc349b2cffe34582c9217708417d57dac497e24227712588ec52fdf4fdd0b08a9d41228ae5eb97857fa95fcee6f863c3c012c36c20689e63b191e00f3679468593d0312ed1e1b069c56a3f8757f058a8938a17ba8d86057914bdfb008d56d0a6dcbe3b0d6b33da6033d13db2adfbdcbb5782117276dcfb2d35a24bef1b1dca557c9d02a7788c225dbebdc944f69968f9ceeeef07af3a7c0491319e0ea04ce320613b185db8432c24fc3f49849b7b9fc945f7704091fad9f3ba1baecdd809cdc4a7966d72f275302cb8f7934b10af0e8aebc18f5452850a35707a401deeacd120e0bd761bd808697ff0a80797f849c16c2bfe2f50c5af6a6c05c539520d8b0ae6a4fdfa43018787c37de6e663cd394df70cdda12ad9cd4c7342148a7c03c40a8d5 当前文章暂不对外可见，请输入访问密码后查看！","link":"/2024/01/09/%E8%87%AA%E5%B7%B1%E9%9C%80%E8%A6%81%E8%AE%B0%E4%BD%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/"},{"title":"Reinforcement Learning Notes 2","text":"Monte Carlo Method、随机梯度下降、时序差分方法 1. 蒙特卡罗方法1.1 基本思想在进行 policy iteration 时，求某个状态 s 采取动作 a 产生的 action value 的时候，我们已知的方法是代入公式 计算。但是这个方法依赖于模型参数已知，我们现在想找到一种方法，在不依赖模型的情况下能求出 action value。 为了达成这个目标，只需要对 policy iteration 每次迭代的第一步做改动。对每一个 pair ，生成充分多的序列 (episode)，对这些序列的 returns 求平均值，用来近似 action value。 1.2 MC Exploring Starts看这样一条 episode： 上述基本方法中，只有每条 episode 开头的 pair 的 action value 会被估计，这显然对数据利用不充分。于是产生了下面的改进： 对每一个 state-action-pair，只取一条 episode，对这个 episode 上出现过的逐个 pair 都做 action value 的计算，而且 policy 是随着每次计算而更新。这相当于，假如又出现了一次，则的估计又增添了样本，policy 也可能随着改动。 1.3 MC - Greedy我们不需要对每一个 pair 生成一条 episode，而是修改 policy 的更新方法，使其兼顾“explore”的功能。具体来说，我们将其改为 代表在 s 可选的 action 数量，位于 [0,1]。等于 0 时，就退化为之前的 greedy 策略；等于 1 时，选取每一个 action 的概率都相同，探索能力最强。 实际使用时，我们在每次迭代都生成足够长的 episode（e.g. 1million），用这条 episode 的每一个 step 一步步去更新 policy，只需数次迭代就可以收敛到较好的策略。 2. 随机梯度下降2.1 背景知识现在有一个数列，要计算它的期望，已知的方法是求平均值，但是在这要等到所有数据准备完毕。为了节约时间，可以采用迭代的 mean estimation 算法： 2.2 RM 算法要求解 的解 ，但是不知道 表达式，只知道给一个输入 ，会有一个输出 ，其中 代表噪音或误差。RM 算法给出如下迭代公式： 2.3 SGD考虑求的极小值。已知 是一个随机变量，但是不知道其分布；另外，已知梯度的采样。SGD 给出 这其实与机器学习的随机梯度下降原理一样。此外，还可以考虑批量梯度下降（BGD）、小批量梯度下降（MBGD）。 3. TD Learning3.1 基础算法我们要解决的问题是，给定一个策略，如何求解对应的 state value 。而且这个问题必须用 model free 的方法求解。 我们现在通过当前策略，生成了一个 episode，TD 算法给出渐进式的估计： 称为 TD Target，为 TD Error。 之所以称为 Target，是因为如下推导： 可以发现与的距离越来越小。 之所以称为 Error，是因为当是精确解时， 值得注意的是，上述算法仅是求一个给定策略的 state value，不涉及策略的优化。 3.2 Sarsa3.2.1 基础算法基础的 TD 算法是求 state value，而 Sarsa 求的是 action value。对一个给定的策略，我们收集当前状态下的 experience：。对 action value 的估计如下： 其中， 是对 的估计。 然后更新 处的策略： 然后跳到 进行下一步迭代。 3.2.2 n-Step Sarsa这是对 Sarsa 和 Monte Carlo 的折中： 回顾 action value 的定义： 是 discounted return。 在 Sarsa 中，它表示为 在 Monte Carlo 中，它表示为 在 n-Step Sarsa 中，它表示为 相应地，估计 action value 的公式改为 Sarsa 作为一种 online 的强化学习算法，每次只需要看下一步的 state 和 action，因此具有比较大的偏差（bias）和比较小的方差（variance）。MC 作为 offline 的算法，需要等待 episode 生成完成才能开始计算，具有较小的 bias 和较大的 variance。 多说一句，variance 较大意味着泛化性差，在这里体现为仅对一条 trajectory 进行拟合。 而 n-Step Sarsa 介于两者之间，需要到时刻才能计算。 Tips：在算法开始时，所有状态-动作对的 Q 值会被赋予初始值（如全零或随机数），并存储在一个表格（Q 表）中。 3.2.3 Q-LearningQ-Learning 的具体公式为 和 Sarsa 的区别在于 TD target 做了修改。 on policy vs off policy TD learning 里面有两个策略的概念：behavior policy，用于生成 experience；target policy，是要优化的对象。当两者相同时，成为 on policy，不同时成为 off policy。 off policy 的优势在于，target policy 要学习的 experience 不必是自己产生的。也就是说，允许从别人的经验中使自己提升。 Sarsa 和 MC 都是 target policy 自己生成数据，然后求出 action value 并优化策略，可见它们是 on policy 的。而 Q-Learning 计算需要，可以从任何一条策略产生，它是 off policy 的。 实际上，我们可以让 behavior 和 target 是同一个策略，来让 Q-Learning 是 on policy 的。这时，策略的更新需要使用方法；如果是 off policy，则使用 Greedy 方法，即每一步更新后的策略都是固定的。","link":"/2025/02/25/Reinforcement-Learning-Notes-2/"},{"title":"Reinforcement Learning Notes 3","text":"值函数近似、策略梯度方法、Actor-Critic 1. 值函数近似之前计算、更新 state/action value 的方法都是基于离散的表格的，现在想用函数来对连续状态进行估计。 总体的思想是，用函数来拟合 state value，更新时，对参数 w 进行更新。 例如，要计算给定策略的 state value，用估计，对于每个 example，更新参数。这只是在求 state value，并没有涉及到 policy update，接下来的方法将直接拟合 action value 并更新参数，这样就可以得到最优策略。 1.1 Sarsa是对于 action value 的拟合。直接给出参数的更新公式： 每步迭代更新一次，然后使用的方法更新策略。 1.2 Q-Learningq-value 的更新与 Sarsa 很像： 策略更新还是用。 1.3 Deep Q-Network基本思想是，是一个神经网络，输入是 s，a，参数是 w。loss function 定义为 不难发现，这个 loss 的本质是 bellman error，如何使它最小化？问题在于这一项。当 w 变化时，让这一项达到最大的 a 也可能会变化。为了消除这种不确定性，DQN 提出了两个技巧。 1. 使用两个神经网络。一个是 main network ，另一个是 target network。target network 的参数不用每一次迭代都更新，在一段时间内我们假设它是固定的，方便求梯度。target network 固定时，梯度就可以写为 这样一来，对于样本，desired output 就是确定的数：，我们就获得了 mini-batch，可以用来最小化 loss function。 2.replay buffer。 DQN 是 off policy 的，也就是说样本可以由其它策略得到，我们选取样本时，不按照 episode 的顺序，而是打混、等概率随机抽取。这样的好处是，不需要生成太长的 episode，能够充分利用样本。 DQN 算法流程如下：对于样本，计算：，然后用这个 batch 来更新 main network 参数，以最小化 loss function。每过 C 个迭代，就更新一次即 target network 的参数。 policy update 等到算法结束后才一次性完成，因为中间并不需要使用 target policy。有了训练好的，就不难知道在某个状态 s 采取哪个动作的 action value 最大，也就很容易得到 optimal policy 了。 2. 策略梯度方法2.1 基本思想之前，在某个状态 s 要采取什么动作，都是通过查表的形式完成。现在想用一个函数（例如神经网络）来直接得出在 s 时采取动作 a 的概率，写为，是网络的参数。 2.2 目标函数如何更新参数？有两个常用的指标：和。第一个比较容易理解，重点看第二个。 ；而可以理解为初始 s 服从某种分布，假设将它写成向量 d，每一个元素就是。而这个向量 d 如何得到呢？假设我们有 state transition matrix ，d 就是的特征向量。 这些指标是的函数，也就是的函数。更新参数的目标就是让这些指标变得尽可能大。值得一提的是，当一个指标最大时，另外的指标也会跟着达到最大，也就是它们的优化是等价的。 有的时候，这两个目标函数也可以写为 第二个式子的第二个等号成立的原因是大数定律，也就是无穷多步以后每个状态的概率已经稳定，而且不考虑 discount。 2.3 REINFORCE接下来，就是使用梯度上升的方法来更新，以最大化目标函数。目标函数梯度的推导过程这里不详细给出，直接看 REINFORCE 算法。 现在有一个网络。在第 k 次迭代时，选取一个，从它开始生成 episode。对每一个状态对，做 ，即从开始算的 action value 最后，进入下一次迭代。 deeper insight 有趣的是， 这实际上是在”合理化”的值（回想梯度上升的公式，不就是的梯度吗），为正数时，经过参数调整后变大；为负数时，经过参数调整后变小。进一步观察，因为的最后一层有 softmax 操作，故分母是正数，那么的正负不就是 action value 的正负吗？ 另外，这个算法还兼顾了探索性。因为当太小时，（假设此时为正数）也会变大，参数更新后将变大，这使得某些可能性较低的情况也会被充分探索。 3.Actor-Criticactor-critic 也是一种策略梯度方法，actor 代表策略的更新，critic 代表策略的评估。基本思想仍然是用梯度上升的方法来最大化目标函数。 3.1 QAC回顾上一节的公式 可以发现，原来是通过采样得到的。现在想直接用神经网络来得到每个（s,a）的 action value。一个基本的算法是，每次迭代时，由策略和环境生成，然后用这个来进行： policy 更新： value 更新： 上述方法称为 QAC。 3.2 A2C目标函数的梯度具有如下性质： 减去有什么好处？这样可以减小方差，进而使采样时样本与期望的误差不至于太大，数学证明这里省略。实际我们常常让 b(s) 等于，即 s 的 state value。 这样一来，上述梯度可以写为 其中定义优势函数。 也可以写出针对某一个具体 episode 的版本： 优势函数还可以进一步改写为。这样做的好处是，原来需要两个网络分别估计 action value 和 state value，现在只需要训练一个网络估计 state value。 这样一来，就得到了 A2C 算法。里面有两个网络： 和 。具体流程是，每次迭代时，由策略和环境生成，然后用这个来进行： 计算优势函数： 更新策略： 更新价值： 可见，这是 online、on-policy 的算法。 deeper insight 越大，说明这一的 “优势” 越大，我们自然想让它被采取的概率变大，而数学上也确实变大。越小，基于充分探索的思想，也会变大。 接下来看一下优势函数评估某个 action 的好处。用这个代替 action value，使其更加关心 相对 大小。这是更加合理的。例如，这个动作产生的 action value 是 0，但是其它的都是-1，那么这个动作无疑是最优的。相反，即使这个动作的 aciton value 是 10，如果其它的都是 100，也不能说这个动作占优。 3.3 DPG3.3.1 重要性采样已知随机变量 X 服从分布，但是有时候不能直接求出它的期望（比如 X 是神经网络的输出）。同时还知道一个服从分布的采样。现在想要间接地求出 X 的期望，需要用到重要性采样： 3.3.2 off policy 梯度下降假设现在有一个 behavior policy ，想要用它的采样来更新 target policy ，以最大化目标函数 根据重要性采样原理，梯度可以写为 减去 baseline，我们就得到了 off policy 的参数更新公式： 定义优势函数后，改写为 其中 这样一来就得到了 off policy 的 A2C 算法，其中 sample 都是由 behavior policy 生成 计算优势函数： 更新策略： 更新价值： 3.3.3 DPG之前对于一个状态，网络会输出采取某个动作的概率。现在想要直接从状态映射到动作本身：。 目标函数选，梯度写为 这样一来，就得到了DPG算法。假设有一个behavior policy ，target policy ，评估函数 ，还是三件套： 优势函数 策略更新 评估更新 注意到这里不需要重要性采样了。","link":"/2025/03/03/Reinforcement-Learning-Notes-3/"}],"tags":[{"name":"RL","slug":"RL","link":"/tags/RL/"},{"name":"thoughts","slug":"thoughts","link":"/tags/thoughts/"}],"categories":[],"pages":[{"title":"about","text":"logo是dropout bear，头像是泽北荣治。喜欢学习机器学习相关的内容。","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}