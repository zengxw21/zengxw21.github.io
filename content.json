{"posts":[{"title":"自己需要记住的一些事","text":"05cfa70b99313444de02ee59215d007d525a4ddde384c9cadfce0102ae417bee127747e34e6dbb8747869b3c2f82c2757435566e505158e1c187f2095b17cfda8b1942e73f5e01efc43411def04f5104454f443f0a82b30531b027fb88514f58af464ffb102cba2c4b6ff393b29fcc7191a8fe883d5c51675f53cba1a8e00615019a631763b806e3cdcac2ec4a735b8450c2646726561c178ef05531836f0849f985790b6b2bf7e6e020fc2af0007ee629a6c90f58f6904d6dccb9731a65c32a2cde4584bc8f0d82377ddcea8af3a60d34ac5d0e30a67a5af673a972640ddd9c427c1f20dae9c70724654c2516f059b379813f35e4aab37d73eab4609d6222e1a8d7f9d5f6408d0d9ba35c0bb8464d9ce5572064e0075ee3536edfcb3598aac9f4b3e0bed2bb12866033992c62afa004274b5aec63016a5deb706a8dc7bd3060cba49afc97b73a91bd8bd356ca34e816f91066d8eb06ad3a8dc51f60dcd40166aead46c734c7eaf68c8bb4205d3ad8bdb6c3f20813e68dffc65bf8e87ff643d31195e8df2e002f8eea1c77120467c7ecc349b2cffe34582c9217708417d57dac497e24227712588ec52fdf4fdd0b08a9d41228ae5eb97857fa95fcee6f863c3c012c36c20689e63b191e00f3679468593d0312ed1e1b069c56a3f8757f058a8938a17ba8d86057914bdfb008d56d0a6dcbe3b0d6b33da6033d13db2adfbdcbb5782117276dcfb2d35a24bef1b1dca557c9d02a7788c225dbebdc944f69968f9ceeeef07af3a7c0491319e0ea04ce320613b185db8432c24fc3f49849b7b9fc945f7704091fad9f3ba1baecdd809cdc4a7966d72f275302cb8f7934b10af0e8aebc18f5452850a35707a401deeacd120e0bd761bd808697ff0a80797f849c16c2bfe2f50c5af6a6c05c539520d8b0ae6a4fdfa43018787c37de6e663cd394df70cdda12ad9cd4c7342148a7c03c40a8d5 当前文章暂不对外可见，请输入访问密码后查看！","link":"/2024/01/09/%E8%87%AA%E5%B7%B1%E9%9C%80%E8%A6%81%E8%AE%B0%E4%BD%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/"},{"title":"Reinforcement Learning Notes","text":"从入门到入坑 1. 贝尔曼公式 考虑这样一个状态转移，agent 在状态 S 采取动作 A，获得 reward R 并转移到下一个状态： 我们计算整条路径上 reward 之和也就是 return 那么如何计算从 s 开始，采用策略获得的 return 的期望，也就是 state value 呢？它等于 分别计算这两项，第一项： 其中 表示在 policy 下，在状态 s 采取动作 a 的概率； 表示在状态 s 采取动作 a 后，获得奖励 reward 的概率。那么上面的式子其实就是求第一步 reward 的期望。 接下来看第二项， 其中 的原因是，整个过程是 Markov 决策，也就是与历史状态无关。而 指的是从 开始，能得到的 return 的期望。这一项求的是从下一个状态开始能得到的 return 的期望。 回头看开始的式子，提取出公共项，就有 注意这里的集合符号，代表这是一组等式，每个 state 都有一个方程。联立这个方程组，我们就能求出每个状态的 state value，进而评估这个策略的好坏。 接下来看 action value 的定义： 它指的是在状态 s，采取动作 a 后，获得 return 的期望。那么我们不难得出，state value 其实就是 action value 的加权平均。 将其与上面得到的式子作比较，发现 这说明 state value 和 action value 其实可以互相求解。","link":"/2025/02/22/Reinforcement-Learning-Notes/"}],"tags":[{"name":"thoughts","slug":"thoughts","link":"/tags/thoughts/"},{"name":"RL","slug":"RL","link":"/tags/RL/"}],"categories":[],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"logo是dropout bear，头像是泽北荣治。喜欢学习机器学习相关的内容。","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}]}